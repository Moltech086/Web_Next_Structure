"use client";
import React, { useState } from "react";
import Modal from "react-bootstrap/Modal";
import { AppIcons } from "@/data/appIcons";
import Image from "next/image";
import ServiceFAQ from "@/components/serviceFAQ/ServiceFAQ";
import { blogConfig } from "@/data/blogs/local-ai-assistants-ollama-n8n-automation";
import Link from "next/link";
import Iconify from "@/components/ui/icons/Iconify";
import CodeBlockCustom from "@/components/codeBlock/CodeBlockCustom";

// code block data
const codeWebhookInput = `{ "prompt": "Your question here" }`;
const codeHttpRequestBody = `{
"model": "llama3:8b",
"prompt": "{{$json.prompt}}",
"stream": false,
"options": { "temperature": 0.2 }
}`;
const codeResponseJson = `{
"answer": "Your AI-generated response",
"model": "llama3:8b"
}`;
const codeCurlTest = `curl -X POST https://your-n8n-host/webhook/ask \\
-H "Content-Type: application/json" \\
-d '{"prompt":"Summarize our Q2 roadmap in two bullet points."}'`;

// Placeholder for a custom CodeBlock component (assuming it exists in the user's project)

const LocalAiAssistantsOllamaN8nAutomation = () => {
  const [showModal, setShowModal] = useState(false);
  const [modalImageSrc, setModalImageSrc] = useState("");
  const [modalImageAlt, setModalImageAlt] = useState("");

  const handleImageClick = (src, alt) => {
    setModalImageSrc(src);
    setModalImageAlt(alt);
    setShowModal(true);
  };

  const handleCloseModal = () => {
    setShowModal(false);
    setModalImageSrc("");
    setModalImageAlt("");
  };
  return (
    <>
      <div className="blog-header-image mt-0">
        <Image
          src={AppIcons.LocalAiAssistantsOllamaN8nAutomation}
          alt=" Building AI Assistants with Ollama and n8n Cover Image"
          loading="lazy"
          width={1280}
          height={640}
          style={{ height: "auto", cursor: "pointer" }}
          onClick={() =>
            handleImageClick(
              AppIcons.LocalAiAssistantsOllamaN8nAutomation,
              " Building AI Assistants with Ollama and n8n Cover Image"
            )
          }
        />
      </div>

      <>
        <h2 id="introduction">
          <strong>
            Building <span>AI Assistants</span> with Ollama and n8n ‚Äî{" "}
            <span>Local</span>, <span>Private</span>, and <span>Automated</span>
          </strong>
        </h2>
        <p>
          Every CTO I meet asks the same two questions: How can we ship AI
          features quickly without leaking data to the cloud? And how do we keep
          costs predictable as usage grows?
        </p>
        <p>
          Hosted LLMs are excellent for prototyping, but they come with
          trade-offs: ongoing token costs, compliance risks, and vendor lock-in.
        </p>
        <p>
          Another path gaining traction is running a{" "}
          <strong>Local AI assistant with Ollama</strong>, orchestrated via the{" "}
          <strong>n8n workflow builder</strong>.
        </p>
        <p>
          The result is a private LLM under your control, integrated with your
          systems, deployable on a laptop, server, or in your data center.
        </p>
        <p>
          In this article, we explain what Ollama and n8n do, why they work well
          together, and provide a step-by-step workflow:
        </p>
        <p>
          user query ‚Üí n8n webhook ‚Üí Ollama API ‚Üí AI response ‚Üí return to user
        </p>
        <p>
          We&apos;ll also cover real-world use cases, architecture tips, and
          common pitfalls, helping you move from idea to production-ready
          prototype.
        </p>
        
        <h3 id="what-is-ollama">
          <strong>
            What Is Ollama? A <span>Local LLM Runner</span> You Control
          </strong>
        </h3>
        <p>
          Ollama is a lightweight, open-source runtime for large language models
          (LLMs) that runs locally or on your servers.
        </p>
        <p>It functions as a local LLM engine with a simple API.</p>
        <p>
          Models like <strong>Llama 3, Mistral, Qwen, and Phi-3</strong> can be
          pulled with a single command, while Ollama handles quantization,
          GPU/CPU acceleration, and streaming responses.
        </p>
        <h4 id="ollama-teams-choose">
          <strong>Why teams choose Ollama:</strong>
        </h4>
        <ul className="pl-4">
          <li>
            <strong>Privacy by default:</strong> Prompts and data never leave
            your environment
          </li>
          <li>
            <strong>Predictable cost:</strong> No per-token API charges; use
            your existing hardware
          </li>
          <li>
            <strong>Flexibility:</strong> Swap models, run multiple models, or
            pin versions
          </li>
          <li>
            <strong>Simplicity:</strong> Clean REST API and CLI that just works
          </li>
        </ul>
        <p>
          It&apos;s the backbone of a private LLM strategy. For regulated data,
          it&apos;s the easiest way to start self-hosted AI.
        </p>
        
        <h3 id="what-is-n8n">
          <strong>
            What Is n8n? The <span>Open-Source Automation Engine</span> for
            Builders
          </strong>
        </h3>
        <p>
          n8n is a self-hosted automation platform‚Äîsimilar to Zapier but fully
          customizable.
        </p>
        <p>
          Its visual workflow builder connects <strong>triggers</strong>{" "}
          (webhooks, Cron, queues) with <strong>actions</strong> (HTTP requests,
          databases, Slack, Gmail, Jira, and more).
        </p>
        <h4 id="n8n-ai-workflows">
          <strong>Why n8n fits AI workflows:</strong>
        </h4>
        <ul className=" pl-4">
          <li>
            <strong>It&apos;s the glue:</strong> Orchestrates data in and out of
            your LLM
          </li>
          <li>
            <strong>Runs anywhere:</strong> Docker, Kubernetes, or a small VM
          </li>
          <li>
            <strong>Extensible:</strong> Code nodes, custom functions, and
            community packages
          </li>
          <li>
            <strong>Governance:</strong> Logs stay in your environment for AI
            privacy and compliance audits
          </li>
        </ul>
        <p>
          Together, <strong>Ollama + n8n</strong> form a complete on-prem AI
          workflow stack: local inference paired with automation across your
          systems.
        </p>
        
        {/*  */}
        <h3 id="benefits">
          <strong>
            Why Ollama + n8n Belongs on Your <span>Roadmap</span> for
            On-premises AI Workflows
          </strong>
        </h3>
        <p>
          Local AI infra combined with automation isn&apos;t just a cost
          play‚Äîit&apos;s a strategy for{" "}
          <strong>control, resilience, and scalability</strong>.
        </p>
        <h4 id="key-benefits">
          <strong>Key benefits:</strong>
        </h4>
        <ul className="pl-4">
          <li>
            <strong>Privacy &amp; security compliance:</strong> Keep prompts,
            documents, and user data inside your VPC or data center. Easily meet
            SOC 2, HIPAA, or internal security requirements.
          </li>
          <li>
            <strong>Cost Efficiency: Avoid Escalating API Costs:</strong> Once
            your hardware is in place, Ollama&apos;s marginal cost per request
            trends toward zero.
          </li>
          <li>
            <strong>Reliability:</strong> Eliminate dependence on external APIs,
            vendor outages, or rate limits. You control the stack‚Äîand your SLAs.
          </li>
          <li>
            <strong>Speed to value:</strong> n8n&apos;s nodes and expressions
            let you prototype in hours and ship in days.
          </li>
          <li>
            <strong>Future-proofing:</strong> Swap models as they improve or add
            retrieval augmentation later, without requiring workflow changes.
          </li>
          <li>
            <strong>Zero external dependency:</strong> Retain full AI
            capabilities while keeping sensitive data local.
          </li>
        </ul>
        <h4 id="insights">
          <strong>Insights:</strong>
        </h4>
        <ul className="pl-4">
          <li>
            <strong>Local AI Execution:</strong> Run your AI assistant entirely
            on your own infrastructure‚Äîno data leaves your environment.
          </li>
          <li>
            <strong>Full Data Control:</strong> Keep all prompts, documents, and
            user information private, meeting internal security or compliance
            needs.
          </li>
          <li>
            <strong>Automated Workflows:</strong> Use n8n to create simple,
            no-code automations that streamline tasks across systems.
          </li>
          <li>
            <strong>Private AI Endpoint:</strong> Integrate your AI assistant
            into any system without exposing data to the cloud.
          </li>
          <li>
            <strong>Custom AI Assistant:</strong> Build an assistant that
            understands your data, responds intelligently, and adapts to your
            workflow.
          </li>
        </ul>
        <h4 id="what-youll-build">
          <strong>What You&apos;ll Build</strong>
        </h4>
        <ul className="pl-4">
          <li>
            <strong>Private API Endpoint:</strong> You&apos;ll create an
            endpoint that listens to user questions and replies with
            AI-generated answers ‚Äî powered entirely by{" "}
            <strong>Ollama running locally</strong>.
          </li>
          <li>
            <strong>Workflow Overview:</strong> User ‚Üí n8n Webhook ‚Üí Ollama API
            ‚Üí Response Back
          </li>
          <li>
            <strong>Example Prompt:</strong> &quot;Summarize our Q2 roadmap in
            two bullet points.&quot;
          </li>
          <li>
            <strong>Response Handling:</strong> Your local Ollama model
            generates a response, and n8n sends it back as structured JSON.
          </li>
        </ul>
        <h4 id="prerequisites">
          <strong>Prerequisites</strong>
        </h4>
        <p>Before we dive in, make sure you have these ready:</p>
        <ul className="pl-4">
          <li>
            <strong>Ollama Installed and Running</strong> : Ollama is a local
            LLM runtime that lets you run models like Llama 3, Mistral, Phi-3,
            and more on your machine. Default API port: <strong>11434</strong>
            Install and pull a model (e.g., Llama 3):{" "}
            <span className="code-inline">ollama pull llama3:8b</span>
          </li>
          <li>
            <strong>n8n Installed and Running</strong> : n8n is an automation
            tool that can run locally or in Docker. It lets you connect APIs,
            process data, and automate tasks without coding. You can use either
            a <strong>local install</strong> or a{" "}
            <strong>Docker container</strong>.
          </li>
        </ul>
        <p>Once both are up, we&apos;re ready to connect them.</p>
        
        <h3 id="setup">
          <strong>
            <span>Step-by-Step Setup</span>: n8n Webhook ‚Üí Ollama API ‚Üí Response
          </strong>
        </h3>
        <p>
          Let&apos;s build this workflow from scratch in <strong>n8n</strong>.
        </p>
        <h4 id="step-1-create-webhook-trigger">
          <strong> Step 1: Create a Webhook Trigger</strong>
        </h4>
        <p>
          This is your AI assistant&apos;s entry point ‚Äî the URL where users or
          apps send their questions.
        </p>
        <p>
          Add a <strong>Webhook node</strong> in n8n.
        </p>
        <h5>
          {" "}
          <strong>Set:</strong>
        </h5>
        <ul className=" pl-4">
          <li>
            <strong>Method</strong>: <strong>POST</strong>
          </li>
          <li>
            <strong>Path</strong>: <strong>/ask</strong>
          </li>
          <li>
            <strong>Response Mode</strong>: <strong>On Received</strong>
          </li>
        </ul>
        <h5>
          {" "}
          <strong>JSON input</strong>:
        </h5>
        <CodeBlockCustom code={codeWebhookInput} language="json" />
        <p>Now, you have a webhook URL like:</p>
        <p>
          <code>https://your-n8n-host/webhook/ask</code>
        </p>
        <p>This acts as your assistant&apos;s public (or internal) endpoint.</p>

        <h4 id="step-2-connect-to-ollama">
          <strong> Step 2: Connect to Ollama via HTTP Request Node</strong>
        </h4>
        <p>
          Next, add an <strong>HTTP Request</strong> node that calls
          Ollama&apos;s local API.
        </p>
        <ul className=" pl-4">
          <li>
            <strong>Method</strong>: <strong>POST</strong>
          </li>
          <li>
            <strong>URL</strong>:{" "}
            <span className="code-inline">
              http://localhost:11434/api/generate
            </span>
          </li>
          <li>
            <strong>Content Type</strong>:{" "}
            <span className="code-inline">application/json</span>
          </li>
        </ul>
        <h5>
          {" "}
          <strong>Body (JSON)</strong>:
        </h5>
        <CodeBlockCustom code={codeHttpRequestBody} language="json" />
        <h5>
          <strong>This means:</strong>
        </h5>
        <ul className=" pl-4">
          <li>
            We&apos;re asking Ollama to use the <strong>Llama 3 (8B)</strong>{" "}
            model.
          </li>
          <li>The prompt comes directly from the user&apos;s request.</li>
          <li>
            <span className="code-inline">stream: false</span> means we&apos;ll
            wait for the full response (simpler for automation).
          </li>
          <li>
            <span className="code-inline">temperature: 0.2</span> keeps the
            answers consistent and factual.
          </li>
        </ul>
        <h4 id="step-4-send-response-back">
          <strong> Step 4: Send the Response Back</strong>
        </h4>
        <p>
          Finally, add a <strong>Respond to Webhook</strong> node. This sends
          the cleaned answer back to the requester.
        </p>
        <p>Use this JSON format:</p>
        <CodeBlockCustom code={codeResponseJson} language="json" />
        <h4 id="step-5-test-it">
          <strong>Step 5: Test It</strong>
        </h4>

        <p>
          You can test it right away using <strong>curl</strong> or any HTTP
          client (like Postman):
        </p>
        <CodeBlockCustom code={codeCurlTest} language="bash" />
        <div className="blog-header-image mt-0">
          <Image
            src={AppIcons.LocalAiAssistantsOllamaN8nAutomation1}
            alt=" Building AI Assistants with Ollama and n8n  "
            loading="lazy"
            width={1280}
            height={640}
            style={{ height: "auto", cursor: "pointer" }}
            onClick={() =>
              handleImageClick(
                AppIcons.LocalAiAssistantsOllamaN8nAutomation1,
                " Building AI Assistants with Ollama and n8n "
              )
            }
          />
        </div>
        <p>Behind the scenes, this happens:</p>
        <ul className=" pl-4">
          <li>n8n receives the prompt via webhook</li>
          <li>
            Sends it to Ollama (
            <span className="code-inline">
              http://localhost:11434/api/generate
            </span>
            )
          </li>
          <li>
            Ollama runs the model and returns: - Launch EU region - Expand
            partner integrations
          </li>
          <li>n8n sends that clean response back to you.</li>
        </ul>
        <p>
          You&apos;ve just built a <strong>fully private AI endpoint</strong>.
        </p>

        

        <h3 id="use-cases">
          <strong>
            <span>Real-World Use Cases</span> You Can Ship This Week
          </strong>
        </h3>
        <ul className="pl-4">
          <li>
            <strong>Auto-summarize documents:</strong> Drop a PDF into a folder
            or S3 bucket. n8n detects the file, extracts text (e.g., via a PDF
            node), and sends it to Ollama for summarization. Output can go to
            Slack, email, or a knowledge base.
          </li>
          <li>
            <strong>Draft emails or support replies:</strong> Connect n8n to
            your help desk. When a ticket arrives, n8n gathers context and asks
            Ollama to draft a reply. Include system prompts for brand voice and
            compliance rules. Agents can review before sending, ensuring AI
            privacy and security.
          </li>
          <li>
            <strong>Private chat assistants for internal use:</strong> Build a
            secure chat UI that posts to your n8n webhook. Add retrieval: n8n
            fetches relevant content from Confluence or a vector database and
            sends it with the prompt to your Private LLM. Ideal for policy
            Q&amp;A, dev onboarding, and internal SOPs.
          </li>
          <li>
            <strong>Meeting notes ‚Üí actionable tasks:</strong> Capture
            transcripts from Zoom or Teams. n8n summarizes via Ollama, then
            creates tasks in Jira, Asana, or GitHub.
          </li>
          <li>
            <strong>Data cleanup and enrichment:</strong> For CSVs or CRM
            entries, n8n parses rows and calls Ollama to normalize fields,
            classify intent, or extract entities. Push cleaned data back to your
            database or analytics pipeline.
          </li>
          <li>
            <strong>Security review assistant:</strong> Feed internal policy
            documents to a retrieval layer. Ask Ollama to evaluate proposed
            changes against policy and highlight gaps.
          </li>
        </ul>

        
        <h3 id="models">
          <strong>
            Choosing <span>Models</span> and Designing for <span>Scale</span>
          </strong>
        </h3>
        <p>
          Not every model fits every task. Start pragmatic, then iterate as
          needs evolve.
        </p>
        <h4 id="model-choices">
          <strong>Model Choices:</strong>
        </h4>
        <ul className=" pl-4">
          <li>
            <strong>General-purpose chat &amp; writing:</strong> Llama 3 8B,
            Mistral 7B, Qwen 2.5 7B
          </li>
          <li>
            <strong>Smaller footprint &amp; precision:</strong> Phi-3 Mini
          </li>
          <li>
            <strong>Code-heavy tasks:</strong> Llama 3 Instruct variants, Qwen
            Coder
          </li>
        </ul>
        <h4 id="hardware-tips">
          <strong>Hardware Tips:</strong>
        </h4>
        <ul className=" pl-4">
          <li>
            Modern CPUs can run quantized 7B models for low-throughput tasks.
          </li>
          <li>
            A single prosumer GPU (e.g., 24‚ÄØGB VRAM) can serve 7B‚Äì13B models for
            team-scale workloads.
          </li>
          <li>
            Quantization reduces memory usage with minimal quality loss for many
            tasks.
          </li>
        </ul>
        <h4 id="architecture-patterns">
          <strong>Architecture Patterns:</strong>
        </h4>
        <ul className="pl-4">
          <li>
            <strong>Retrieval-Augmented Generation (RAG):</strong> Use n8n to
            pull relevant documents or vectors before calling Ollama. Pass the
            snippets with the prompt for grounded answers.
          </li>
          <li>
            <strong>Prompt templates &amp; system prompts:</strong> Standardize
            tone, compliance, and formatting. Store them in n8n credentials or
            environment variables.
          </li>
          <li>
            <strong>Caching:</strong> Cache frequent answers or embeddings to
            reduce load.
          </li>
          <li>
            <strong>Streaming:</strong> For chat UIs, set{" "}
            <span className="code-inline">&quot;stream&quot;: true</span> to
            reduce perceived latency.
          </li>
          <li>
            <strong>Observability:</strong> Log prompt and response metadata for
            QA and iteration‚Äîwithout storing sensitive content unless necessary.
          </li>
        </ul>
        
        {/* <h3 id="pitfalls">
          <strong>
            <span>Common Pitfalls</span> and How to <span>Avoid Them</span>
          </strong>
        </h3>
        <ul className=" pl-4">
          <li>
            <h5>
              <strong>Choosing a model that&apos;s too large:</strong>
            </h5>
            <p>
              Start with 7B‚Äì8B. Monitor output quality and scale up only if
              necessary.
            </p>
          </li>
          <li>
            <h5>
              <strong>No guardrails:</strong>
            </h5>
            <p>
              Always include a system prompt with rules (e.g., ‚ÄúIf unsure, ask
              for clarification‚Äù) to reduce hallucinations.
            </p>
          </li>
          <li>
            <h5>
              <strong>Ignoring timeouts and retries:</strong>
            </h5>
            <p>
              Configure timeouts in n8n HTTP nodes and implement retries with
              backoff to handle transient load.
            </p>
          </li>
          <li>
            <h5>
              <strong>Logging sensitive data:</strong>
            </h5>
            <p>
              Redact fields or avoid storing raw prompts/responses. Scrub data
              in a Function node before persisting.
            </p>
          </li>
          <li>
            <h5>
              <strong>Neglecting dependency isolation:</strong>
            </h5>
            <p>
              Use Docker or containers to pin model versions and isolate
              services.
            </p>
          </li>
          <li>
            <h5>
              <strong>Poor networking setup:</strong>
            </h5>
            <p>
              In Docker, ensure n8n and Ollama share a network. Use service
              names instead of <span className="code-inline">localhost</span>.
            </p>
          </li>
          <li>
            <h5>
              <strong>Skipping evaluation:</strong>
            </h5>
            <p>
              Maintain a small test set of prompts and expected outputs. Track
              accuracy and hallucination rates like any other product metric.
            </p>
          </li>
        </ul> */}
        <h3 id="pitfalls">
  <strong>
    <span>Common Pitfalls</span> and How to <span>Avoid Them</span>
  </strong>
</h3>
<ul className="pl-4">
  <li>
    <strong>Choosing a model that&apos;s too large:</strong> Start with 7B‚Äì8B. Monitor output quality and scale up only if necessary.
  </li>
  <li>
    <strong>No guardrails:</strong> Always include a system prompt with rules (e.g., ‚ÄúIf unsure, ask for clarification‚Äù) to reduce hallucinations.
  </li>
  <li>
    <strong>Ignoring timeouts and retries:</strong> Configure timeouts in n8n HTTP nodes and implement retries with backoff to handle transient load.
  </li>
  <li>
    <strong>Logging sensitive data:</strong> Redact fields or avoid storing raw prompts/responses. Scrub data in a Function node before persisting.
  </li>
  <li>
    <strong>Neglecting dependency isolation:</strong> Use Docker or containers to pin model versions and isolate services.
  </li>
  <li>
    <strong>Poor networking setup:</strong> In Docker, ensure n8n and Ollama share a network. Use service names instead of <span className="code-inline">localhost</span>.
  </li>
  <li>
    <strong>Skipping evaluation:</strong> Maintain a small test set of prompts and expected outputs. Track accuracy and hallucination rates like any other product metric.
  </li>
</ul>

        
        <h3 id="moltech">
          <strong>
            From <span>Prototype to Production</span> with Moltech
          </strong>
        </h3>
        <p>
          If you&apos;re thinking,{" "}
          <strong>‚ÄúWe can ship this in a week,‚Äù</strong> you&apos;re not wrong.
        </p>
        <p>
          The stack is mature, the tools are open-source, and the path from idea
          to impact is short.
        </p>
        <p>
          Moltech specializes in open-source AI integration and AI-powered
          automation. We design on-prem AI workflows, build n8n automations
          around the Ollama API, and implement safeguards for privacy and
          security.
        </p>
        <p>
          Whether you&apos;re piloting a Local AI assistant, designing a
          retrieval layer, or scaling on-prem AI workflows across teams, we can
          help.
        </p>
        <ul className=" pl-4">
          <li>
            Explore <strong>Moltech AI Integration Services</strong> to
            architect your stack.
          </li>
          <li>
            Visit <strong>Moltech Automation Consulting</strong> to move from
            prototype to production.
          </li>
          <li>
            For regulated environments, see{" "}
            <strong>Moltech Security-by-Design AI</strong> for hardened
            workflows, auditability, and compliance support.
          </li>
        </ul>
        
        <h3 id="conclusion">
          <strong>
            <span>Conclusion</span>: <span>Private</span>, <span>Powerful</span>
            , and Ready for Your <span>Roadmap</span>
          </strong>
        </h3>
        <p>
          Ollama provides a private LLM engine, and n8n delivers orchestration.
        </p>
        <p>
          Together, they enable on-prem AI workflows that are fast to build,
          safe to run, and cost-effective to scale.
        </p>
        <p>
          Automate document summaries, draft support replies, and deploy
          internal chat assistants‚Äîall without sending data to external APIs.
        </p>
        <p>
          If you&apos;re serious about <strong>Local AI assistants</strong> and{" "}
          <strong>n8n automation</strong>, now is the time to act.
        </p>
        <p>
          Start with the simple webhook-to-Ollama pattern, layer in retrieval as
          needed, and harden security as you grow.
        </p>
        <p>
          When you&apos;re ready for design reviews, performance tuning, or a
          full production rollout, Moltech is ready to guide you every step of
          the way.
        </p>
      </>

      <div className="separate-cta ">
        {/* <div className="icon-sec">
              <Image
                src={AppIcons.ctaBgCommonIcon}
                alt="CTA Icon Image"
                loading="lazy"
                priority={false}
                className="img-fluid"
              />
            </div> */}
        <div className="desc-sec ">
          <p>
            üëâAt Moltech, we help you run private LLMs securely with Ollama. Our
            team designs on-prem AI systems that balance speed, compliance, and
            control. Explore Secure On-Prem AI Deployment and Edge AI
            Architecture with us today.
          </p>

          <div className="button-sec">
            <Link rel="canonical" className="service-btn" href="/contact-us">
              <span>Let&apos;s Connect</span>
              <Iconify icon="weui:arrow-filled" width={20} />
            </Link>
          </div>
        </div>
      </div>
      <div id="faqs" className="blog-faq mt-5">
        <ServiceFAQ
          items={blogConfig.faqData.faqItems}
          title={blogConfig.faqData.faqTitle}
        />
      </div>
      {/* Image Modal */}
      <Modal show={showModal} onHide={handleCloseModal} centered size="lg">
        <Modal.Header closeButton>
          <Modal.Title>{modalImageAlt}</Modal.Title>
        </Modal.Header>
        <Modal.Body className="text-center">
          <Image src={modalImageSrc} alt={modalImageAlt} fluid />
        </Modal.Body>
      </Modal>
    </>
  );
};

export default LocalAiAssistantsOllamaN8nAutomation;
